import os
import streamlit as st
import psycopg2
from psycopg2.extras import RealDictCursor
# from sentence_transformers import SentenceTransformer # util is not used directly
# import numpy as np # Not explicitly used
from dotenv import load_dotenv
from streamlit_searchbox import st_searchbox # å¯¼å…¥ streamlit-searchbox
import requests

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "bge-m3")
EMBEDDING_API_BASE = os.getenv("OLLAMA_BASE_URL", "http://10.24.116.15:8998")
EMBEDDING_API_ENDPOINT = os.getenv("EMBEDDING_API_ENDPOINT", "embed")

# åˆå§‹åŒ–PostgreSQLè¿æ¥ (ä½¿ç”¨Streamlitç¼“å­˜)
@st.cache_resource
def init_postgres():
    conn = psycopg2.connect(
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        host=DB_HOST,
        port=DB_PORT
    )
    return conn

# # åŠ è½½è¯­ä¹‰æœç´¢æ¨¡å‹ (ä½¿ç”¨Streamlitç¼“å­˜)
# @st.cache_resource
# def load_model():
#     # è­¦å‘Šï¼š'all-MiniLM-L6-v2' ç”Ÿæˆ384ç»´å‘é‡ã€‚
#     # å¦‚æœæ‚¨çš„æ•°æ®åº“ 'summary_ai_embedding' åˆ—å­˜å‚¨çš„æ˜¯1536ç»´å‘é‡ï¼Œè¿™å°†ä¸åŒ¹é…ã€‚
#     # è¯·ç¡®ä¿æ¨¡å‹ä¸æ•°æ®åº“ä¸­çš„å‘é‡ç»´åº¦ä¸€è‡´ã€‚
#     return SentenceTransformer('all-MiniLM-L6-v2')

# é€šç”¨SQLæŸ¥è¯¢å‡½æ•°
def fetch_papers_general(sql_query, params=None):
    conn = init_postgres()
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(sql_query, params)
        papers = cur.fetchall()
    return papers

def safe_format_tsquery_input(query_text: str, prefix_match: bool = True) -> str:
    """
    å®‰å…¨åœ°å°†ç”¨æˆ·è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºPostgreSQL to_tsqueryå‡½æ•°å¯ä»¥ç†è§£çš„å­—ç¬¦ä¸²ã€‚
    å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œå¹¶å°†å•è¯ç”¨ '&' (AND) è¿æ¥ã€‚
    æ”¯æŒå¯¹æ¯ä¸ªå•è¯è¿›è¡Œå‰ç¼€åŒ¹é… (e.g., 'exampl:*')ã€‚
    """
    words = query_text.strip().split()
    if not words:
        return ''
    
    processed_words = []
    for word in words:
        # ç§»é™¤æˆ–è½¬ä¹‰å¯èƒ½å¹²æ‰°tsqueryçš„å­—ç¬¦ã€‚è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªä¿ç•™å­—æ¯å’Œæ•°å­—ã€‚
        # æ›´å¤æ‚çš„åœºæ™¯å¯èƒ½éœ€è¦æ›´å®Œå–„çš„æ¸…ç†é€»è¾‘ã€‚
        safe_word = ''.join(char for char in word if char.isalnum())
        if safe_word:
            if prefix_match:
                processed_words.append(safe_word + ":*")
            else:
                processed_words.append(safe_word)
    
    if not processed_words:
        return ''
        
    # ç”¨ AND (&) è¿æ¥æ‰€æœ‰å¤„ç†è¿‡çš„è¯è¯­
    return " & ".join(processed_words)

# ä¸ºæœç´¢æ¡†è·å–å»ºè®®çš„å‡½æ•°
def fetch_search_suggestions(searchterm: str) -> list[str]:
    """
    å½“ç”¨æˆ·åœ¨streamlit-searchboxä¸­è¾“å…¥æ—¶ï¼Œæ­¤å‡½æ•°è¢«è°ƒç”¨ä»¥è·å–å»ºè®®ã€‚
    """
    if not searchterm or len(searchterm) < 3: # è‡³å°‘è¾“å…¥2ä¸ªå­—ç¬¦æ‰å¼€å§‹å»ºè®®
        return []
    conn = init_postgres()
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            # åŸºäºæ ‡é¢˜è¿›è¡Œå»ºè®®ï¼Œä½¿ç”¨å…¨æ–‡æœç´¢çš„å‰ç¼€åŒ¹é…è·å–æ›´ç›¸å…³çš„å»ºè®®
            # ts_query_str = searchterm.strip() + ":*" # ç®€å•å‰ç¼€åŒ¹é…
            ts_query_str = safe_format_tsquery_input(searchterm, prefix_match=True)
            if not ts_query_str:
                return []

            # æˆ‘ä»¬ä¸»è¦æ ¹æ®æ ‡é¢˜æˆ–è€…AIæ€»ç»“æ¥æä¾›å»ºè®®
            sql = """
                SELECT title
                FROM papers
                WHERE fts_document @@ to_tsquery('english', %s)
                ORDER BY ts_rank(fts_document, to_tsquery('english', %s)) DESC
                LIMIT 7; 
            """
            # å¦‚æœåªæƒ³åœ¨æ ‡é¢˜ä¸­æœç´¢å»ºè®®ï¼š
            # WHERE to_tsvector('english', title) @@ to_tsquery('english', %s)
            # ORDER BY ts_rank(to_tsvector('english', title), to_tsquery('english', %s)) DESC
            
            cur.execute(sql, (ts_query_str, ts_query_str))
            suggestions = cur.fetchall()
            return [s['title'] for s in suggestions]
    except Exception as e:
        print(f"è·å–æœç´¢å»ºè®®æ—¶å‡ºé”™: {e}") # åœ¨åå°æ‰“å°é”™è¯¯
        return []

def get_embedding_via_api(text: str, model: str = EMBEDDING_MODEL, endpoint: str = EMBEDDING_API_ENDPOINT) -> list[float]:
    url = f"{EMBEDDING_API_BASE}/api/{endpoint}"
    payload = {"model": model, "input": text}
    try:
        resp = requests.post(url, json=payload, timeout=60)
        resp.raise_for_status()
        result = resp.json()
        if "embedding" in result:
            return result["embedding"]
        elif "embeddings" in result and isinstance(result["embeddings"], list):
            return result["embeddings"][0]
        else:
            st.error("Embedding API è¿”å›æ ¼å¼å¼‚å¸¸")
            return None
    except Exception as e:
        st.error(f"è·å– embedding å¤±è´¥: {e}")
        return None

# ä½¿ç”¨pgvectorè¿›è¡Œè¯­ä¹‰æœç´¢
def semantic_search_db(query_text: str, top_k=10):
    conn = init_postgres()
    query_embedding = get_embedding_via_api(query_text)
    if not query_embedding:
        return []
    sql = """
    SELECT id, title, abstract, primary_category_code, pdf_url, summary_ai,
           detailed_review_ai, arxiv_published_at,
           (title_abstract_embedding <=> %s) AS distance 
    FROM papers
    WHERE title_abstract_embedding IS NOT NULL
    ORDER BY distance ASC
    LIMIT %s;
    """
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(sql, (query_embedding, top_k))
        results = cur.fetchall()
    return results

# Streamlit åº”ç”¨ä¸»å‡½æ•°
def main():
    st.set_page_config(layout="wide", page_title="arXiv Paper Search", page_icon="ğŸ“š")
    st.title("ğŸ“š arXiv è®ºæ–‡æ™ºèƒ½æ£€ç´¢")
    st.write("ä½¿ç”¨å…³é”®è¯æˆ–è¯­ä¹‰æœç´¢ arXiv è®ºæ–‡ã€‚åœ¨ä¸‹æ–¹æœç´¢æ¡†ä¸­è¾“å…¥å†…å®¹å³å¯çœ‹åˆ°å»ºè®®ã€‚")

    # # åŠ è½½æ¨¡å‹ (æ•°æ®åº“è¿æ¥åœ¨éœ€è¦æ—¶ç”±å‡½æ•°å†…éƒ¨è·å–)
    # model = load_model()

    # --- ä¾§è¾¹æ  ---
    with st.sidebar:
        st.header("æœç´¢é€‰é¡¹")
        search_type = st.selectbox(
            "é€‰æ‹©æœç´¢ç±»å‹:",
            ["å…³é”®è¯æœç´¢", "è¯­ä¹‰æœç´¢"],
            key="search_type_selector"
        )

        # ä½¿ç”¨ st_searchbox æ›¿ä»£ st.text_input
        # `key` å¯¹äº st_searchbox æ˜¯å¿…éœ€çš„ä¸”å¿…é¡»å”¯ä¸€
        # `default_use_searchterm=True` è¡¨ç¤ºå¦‚æœç”¨æˆ·è¾“å…¥æ–‡æœ¬ä½†æœªé€‰æ‹©ä»»ä½•å»ºè®®ï¼Œåˆ™è¿”å›ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        selected_search_term = st_searchbox(
            search_function=fetch_search_suggestions, # è·å–å»ºè®®çš„å‡½æ•°
            key="paper_searchbox_sidebar",
            placeholder="è¾“å…¥è®ºæ–‡æ ‡é¢˜æˆ–å…³é”®è¯...",
            label="æœç´¢æŸ¥è¯¢:",
            default_use_searchterm=True, 
            clear_on_submit=False,      # é€‰æ‹©åä¸æ¸…ç©ºè¾“å…¥æ¡†ï¼Œä¾¿äºç”¨æˆ·çœ‹åˆ°å½“å‰æœç´¢è¯
            rerun_on_update=True,       # å½“å»ºè®®æ›´æ–°æ—¶é‡æ–°è¿è¡Œ
            debounce=600,               # å»¶è¿Ÿ600msæ‰§è¡Œå›è°ƒï¼Œé¿å…è¾“å…¥è¿‡å¿«æ—¶é¢‘ç¹è¯·æ±‚
        )
        
        if selected_search_term:
            st.caption(f"å½“å‰æœç´¢: '{selected_search_term}'")
        
        st.markdown("---")
        st.info("æç¤ºï¼šè¯­ä¹‰æœç´¢æ›´æ“…é•¿ç†è§£å¥å­å«ä¹‰ï¼Œå…³é”®è¯æœç´¢åˆ™ç²¾ç¡®åŒ¹é…å­—è¯ã€‚")

    # --- ç»“æœå±•ç¤ºåŒºåŸŸ ---
    results_container = st.container()

    if selected_search_term:
        if search_type == "å…³é”®è¯æœç´¢":
            results_container.subheader(f"ğŸ” \"{selected_search_term}\" çš„å…³é”®è¯æœç´¢ç»“æœ")
            
            # å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºé€‚åˆto_tsqueryçš„æ ¼å¼
            ts_query_formatted = safe_format_tsquery_input(selected_search_term, prefix_match=False) # éå‰ç¼€å…¨è¯åŒ¹é…

            if not ts_query_formatted:
                results_container.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„å…³é”®è¯ã€‚")
                return # æå‰é€€å‡ºï¼Œä¸æ‰§è¡Œæœç´¢

            # ä½¿ç”¨fts_documentè¿›è¡Œå…¨æ–‡æ£€ç´¢
            sql = """
            SELECT id, title, abstract, primary_category_code, pdf_url, summary_ai, 
                   detailed_review_ai, arxiv_published_at,
                   ts_rank(fts_document, to_tsquery('english', %s)) as rank
            FROM papers
            WHERE fts_document @@ to_tsquery('english', %s)
            ORDER BY rank DESC, arxiv_published_at DESC
            LIMIT 10;
            """
            papers = fetch_papers_general(sql, (ts_query_formatted, ts_query_formatted))
            
            if papers:
                for i, paper in enumerate(papers):
                    with results_container.expander(f"**{i+1}. {paper['title']}** (ç›¸å…³åº¦: {paper.get('rank',0):.2f})"):
                        st.markdown(f"**AI ä¸€å¥è¯æ€»ç»“**: {paper.get('summary_ai', 'N/A')}")
                        st.markdown(f"**AI æ³›è¯»æŠ¥å‘Šæ‘˜è¦**: _{paper.get('detailed_review_ai', 'N/A')}..._") # æ˜¾ç¤ºéƒ¨åˆ†æ³›è¯»æŠ¥å‘Š
                        st.markdown(f"**ArXiv æ‘˜è¦**: _{paper.get('abstract', 'N/A')}..._") # æ˜¾ç¤ºéƒ¨åˆ†æ‘˜è¦
                        col1, col2, col3 = st.columns(3)
                        col1.metric("ä¸»è¦åˆ†ç±»", paper.get('primary_category_code', 'N/A'))
                        col2.metric("å‘è¡¨æ—¥æœŸ", paper.get('arxiv_published_at').strftime('%Y-%m-%d') if paper.get('arxiv_published_at') else 'N/A')
                        if paper.get('pdf_url'):
                            col3.link_button("é˜…è¯»PDFåŸæ–‡", paper['pdf_url'], use_container_width=True)
            else:
                results_container.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ã€‚")
        elif search_type == "è¯­ä¹‰æœç´¢":
            results_container.subheader(f"ğŸ§  \"{selected_search_term}\" çš„è¯­ä¹‰æœç´¢ç»“æœ")
            results = semantic_search_db(selected_search_term, top_k=10)
            if results:
                for i, paper in enumerate(results):
                    similarity_score = 1 - paper.get('distance', 1.0)
                    with results_container.expander(f"**{i+1}. {paper['title']}** (è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity_score:.3f})"):
                        st.markdown(f"**AI ä¸€å¥è¯æ€»ç»“**: {paper.get('summary_ai', 'N/A')}")
                        st.markdown(f"**AI æ³›è¯»æŠ¥å‘Šæ‘˜è¦**: _{paper.get('detailed_review_ai', 'N/A')[:300]}..._")
                        st.markdown(f"**ArXiv æ‘˜è¦**: _{paper.get('abstract', 'N/A')[:300]}..._")
                        col1, col2, col3 = st.columns(3)
                        col1.metric("ä¸»è¦åˆ†ç±»", paper.get('primary_category_code', 'N/A'))
                        col2.metric("å‘è¡¨æ—¥æœŸ", paper.get('arxiv_published_at').strftime('%Y-%m-%d') if paper.get('arxiv_published_at') else 'N/A')
                        if paper.get('pdf_url'):
                            col3.link_button("é˜…è¯»PDFåŸæ–‡", paper['pdf_url'], use_container_width=True)
            else:
                results_container.info("æ²¡æœ‰æ‰¾åˆ°è¯­ä¹‰ä¸Šç›¸ä¼¼çš„è®ºæ–‡ã€‚")
    else:
        results_container.info("è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥æœç´¢è¯å¼€å§‹æ£€ç´¢ã€‚")

if __name__ == "__main__":
    main()